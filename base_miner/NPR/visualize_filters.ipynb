{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d64586-c0be-4bdd-bb35-69389f59faab",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_loader), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b8b39-4b8b-4d49-b868-41c24844dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = model.model\n",
    "\n",
    "\n",
    "model_weights = [] # we will save the conv layer weights in this list\n",
    "conv_layers = [] # we will save the 49 conv layers in this list\n",
    "# get all the model children as list\n",
    "model_children = list(resnet_model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf0f155-fb5f-4b88-99b1-05b3d9b0d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "counter = 0 \n",
    "# append all the conv layers and their respective weights to the list\n",
    "for i in range(len(model_children)):\n",
    "    if type(model_children[i]) == nn.Conv2d:\n",
    "        counter += 1\n",
    "        model_weights.append(model_children[i].weight)\n",
    "        conv_layers.append(model_children[i])\n",
    "    elif type(model_children[i]) == nn.Sequential:\n",
    "        for j in range(len(model_children[i])):\n",
    "            for child in model_children[i][j].children():\n",
    "                if type(child) == nn.Conv2d:\n",
    "                    counter += 1\n",
    "                    model_weights.append(child.weight)\n",
    "                    conv_layers.append(child)\n",
    "print(f\"Total convolutional layers: {counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2abb35-8407-4355-a402-d89337e89a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the conv layers and the respective weights\n",
    "for weight, conv in zip(model_weights, conv_layers):\n",
    "    # print(f\"WEIGHT: {weight} \\nSHAPE: {weight.shape}\")\n",
    "    print(f\"CONV: {conv} ====> SHAPE: {weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c212c0-6717-467b-afdf-b08b43dd5182",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = True\n",
    "if not skip:\n",
    "    plt.figure(figsize=(20, 17))\n",
    "    for i, filter in enumerate(model_weights[0]):\n",
    "        plt.subplot(8, 8, i+1) # (8, 8) because in conv0 we have 7x7 filters and total of 64 (see printed shapes)\n",
    "        plt.imshow(filter[0, :, :].cpu().detach(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ee380c-94d7-4018-9dc7-56b9d87f2002",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        keep_idx = [i for i, b in enumerate(batch) if b[0].shape[0] == batch[0][0].shape[0]][2:]\n",
    "        # batch = np.array(batch)\n",
    "        images = [b[0] for i, b in enumerate(batch) if i in keep_idx]\n",
    "        labels = [torch.tensor(b[1]) for i, b in enumerate(batch) if i in keep_idx]\n",
    "        for image, label in zip(images, labels):\n",
    "            image, label = image.to('cuda').float(), label.to('cuda').float()\n",
    "\n",
    "            # pass the image through all the layers\n",
    "            results = [conv_layers[0](image.unsqueeze(0))]\n",
    "            for i in range(1, len(conv_layers)):\n",
    "                # pass the result from the last layer to the next layer\n",
    "                results.append(conv_layers[i](results[-1]))\n",
    "            # make a copy of the `results`\n",
    "            outputs = results\n",
    "            break\n",
    "        break\n",
    "        \n",
    "# visualize 64 features from each layer \n",
    "# (although there are more feature maps in the upper layers)\n",
    "for num_layer in range(len(outputs)):\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    layer_viz = outputs[num_layer][0, :, :, :].cpu()\n",
    "    layer_viz = layer_viz.data\n",
    "    print(layer_viz.size())\n",
    "    for i, filter in enumerate(layer_viz):\n",
    "        if i == 64: # we will visualize only 8x8 blocks from each layer\n",
    "            break\n",
    "        plt.subplot(8, 8, i + 1)\n",
    "        plt.imshow(filter, cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "    #print(f\"Saving layer {num_layer} feature maps...\")\n",
    "    #plt.savefig(f\"../outputs/layer_{num_layer}.png\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmsn",
   "language": "python",
   "name": "bmsn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
