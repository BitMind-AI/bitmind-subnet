{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b9847b-6e65-43da-aa1d-3976725bda26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Standard library imports\n",
    "import sys\n",
    "import gc\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from multiprocessing import Pool, cpu_count, current_process, get_context, Manager\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "import warnings\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers.models.modeling_outputs import Transformer2DModelOutput\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToPILImage\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration, BlipProcessor, logging as transformers_logging, pipeline\n",
    "\n",
    "# Suppress TensorFlow logging before import\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 1: filter out INFO, 2: filter out WARNING, 3: filter out ERROR\n",
    "import tensorflow as tf\n",
    "\n",
    "# Local/application-specific imports\n",
    "import bittensor as bt\n",
    "from bitmind.constants import DATASET_META, IMAGE_ANNOTATION_MODEL, PROMPT_GENERATOR_ARGS, PROMPT_GENERATOR_NAMES, DIFFUSER_ARGS, DIFFUSER_NAMES\n",
    "from bitmind.image_dataset import ImageDataset\n",
    "from utils import image_utils\n",
    "from multiprocessing_tasks import generate_images_for_chunk, worker_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af5467-4f78-464d-9dd7-a02d51f24e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#Default log settings\n",
    "transformers_level = logging.getLogger(\"transformers\").getEffectiveLevel()\n",
    "huggingface_hub_level = logging.getLogger(\"huggingface_hub\").getEffectiveLevel()\n",
    "\n",
    "#Suppress logs\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"huggingface_hub\").setLevel(logging.ERROR)\n",
    "processor = AutoProcessor.from_pretrained(IMAGE_ANNOTATION_MODEL)\n",
    "# by default `from_pretrained` loads the weights in float32\n",
    "# we load in float16 instead to save memory\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(IMAGE_ANNOTATION_MODEL, torch_dtype=torch.float16) \n",
    "model.to(device)\n",
    "\n",
    "#Restore log settings\n",
    "logging.getLogger(\"transformers\").setLevel(transformers_level)\n",
    "logging.getLogger(\"huggingface_hub\").setLevel(huggingface_hub_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd53514-8d04-457d-85b7-5f08b0fda441",
   "metadata": {},
   "source": [
    "### Real Image to Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402a888f-868a-4d2e-a252-9358766a5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"A picture of\",\n",
    "    \"The setting is\",\n",
    "    \"The background is\",\n",
    "    \"The image type/style is\"\n",
    "    # \"the background is\",\n",
    "    # \"The color(s) are\",\n",
    "    # \"The texture(s) are\",\n",
    "    # \"The emotion/mood is\",\n",
    "    # \"The image medium is\",\n",
    "    # \"The image style is\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a0686e-5141-4d0f-8779-aa08fb5ff1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_description(image: PIL.Image.Image, use_prompts: bool = True, verbose: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Generates a description for a given image using a sequence of prompts.\n",
    "\n",
    "    Parameters:\n",
    "    image (PIL.Image.Image): The image for which to generate a text description (string).\n",
    "    use_prompts (bool) : Determines whether to use prompt input for BLIP-2 image-to-text generation.\n",
    "    verbose (bool): If True, prints the prompts and answers during processing. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated description for the image.\n",
    "    \"\"\"\n",
    "    if not verbose: transformers_logging.set_verbosity_error() # Only display error messages (no warnings)\n",
    "    description = \"\"\n",
    "    if not use_prompts:\n",
    "        inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        description += generated_text\n",
    "    else:\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            # Append prompt to description to build context history\n",
    "            description += prompt + ' '\n",
    "            inputs = processor(image, text=description, return_tensors=\"pt\").to(device, torch.float16)\n",
    "            generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "            answer = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "            if answer:\n",
    "                # Append answer to description to build context history\n",
    "                description += answer\n",
    "            else:\n",
    "                description = description[:-len(prompt) - 1]  # Remove the last prompt if no answer is generated\n",
    "            if verbose:\n",
    "                print(f\"{i}. Prompt: {prompt}\")\n",
    "                print(f\"{i}. Answer: {answer}\")\n",
    "    if not verbose: transformers_logging.set_verbosity_info() # Restore transformer warnings\n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7d73f2-c4cd-40f9-b731-ec3780140d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def set_logging_level(verbose: int):\n",
    "    level = logging.WARNING if verbose == 0 else logging.INFO if verbose < 3 else logging.DEBUG\n",
    "    logging.getLogger().setLevel(level)\n",
    "\n",
    "def ensure_save_path(path: str) -> str:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "\n",
    "def create_annotation_dataset_directory(base_path: str, dataset_name: str) -> str:\n",
    "    safe_name = dataset_name.replace(\"/\", \"_\")\n",
    "    full_path = os.path.join(base_path, safe_name)\n",
    "    if not os.path.exists(full_path):\n",
    "        os.makedirs(full_path)\n",
    "    return full_path\n",
    "\n",
    "def generate_annotation(image_id,\n",
    "                        dataset_name: str,\n",
    "                        image: PIL.Image.Image,\n",
    "                        original_dimensions: tuple,\n",
    "                        resize: bool,\n",
    "                        resize_dim: int,\n",
    "                        use_prompts: bool,\n",
    "                        verbose: int):\n",
    "    \"\"\"\n",
    "    Generate a text annotation for a given image.\n",
    "\n",
    "    Parameters:\n",
    "    image_id (int or str): The identifier for the image within the dataset.\n",
    "    dataset_name (str): The name of the dataset the image belongs to.\n",
    "    image (PIL.Image.Image): The image object that requires annotation.\n",
    "    original_dimensions (tuple): Original dimensions of the image as (width, height).\n",
    "    resize (bool): Allow image downsizing to maximum dimensions of (1280, 1280).\n",
    "    verbose (int): Verbosity level. If greater than 1, prints image resize message.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing the annotation data.\n",
    "    \"\"\"\n",
    "    image_to_process = image.copy()\n",
    "    if resize:\n",
    "        image_to_process = image_utils.resize_image(image_to_process, resize_dim, resize_dim)\n",
    "        if verbose > 1 and image_to_process.size != image.size:\n",
    "            print(f\"Resized {image_id}: {image.size} to {image_to_process.size}\")\n",
    "\n",
    "    description = generate_description(image_to_process, use_prompts, verbose > 2)\n",
    "    annotation = {\n",
    "        'description': description,\n",
    "        'original_dataset': dataset_name,\n",
    "        'original_dimensions': f\"{original_dimensions[0]}x{original_dimensions[1]}\",\n",
    "        'index': image_id\n",
    "    }\n",
    "    return annotation\n",
    "\n",
    "def save_annotation(dataset_dir: str, image_id, annotation: dict, verbose: int):\n",
    "    \"\"\"\n",
    "    Save a text annotation to a JSON file if it doesn't already exist.\n",
    "\n",
    "    Parameters:\n",
    "    dataset_dir (str): The directory where the annotation file will be saved.\n",
    "    image_id (int or str): The identifier for the image within the dataset.\n",
    "    annotation (dict): Annotation data to be saved.\n",
    "    verbose (int): Verbosity level. If greater than 0, it prints messages during processing.\n",
    "\n",
    "    Returns:\n",
    "    int: Returns 0 if the annotation is successfully saved, -1 if the annotation file already exists.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(dataset_dir, f\"{image_id}.json\")\n",
    "    if os.path.exists(file_path):\n",
    "        if verbose > 0: print(f\"Annotation for {image_id} already exists - Skipping\")\n",
    "        return -1  # Skip this image as it already has an annotation\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(annotation, f, indent=4)\n",
    "        if verbose > 0: print(f\"Created {file_path}\")\n",
    "\n",
    "    return 0\n",
    "\n",
    "def process_image(dataset_dir: str,\n",
    "                  image_info: dict,\n",
    "                  dataset_name: str,\n",
    "                  image_index: int,\n",
    "                  resize: bool,\n",
    "                  resize_dim : int,\n",
    "                  use_prompts: bool,\n",
    "                  verbose: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Process an image from a dataset and generate annotations.\n",
    "\n",
    "    This function handles the processing of an individual image from the given dataset.\n",
    "    It generates annotations based on the provided parameters, saves the annotation\n",
    "    in JSON format, and returns the annotation along with the time taken for processing.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_dir (str): The directory where the annotation will be stored.\n",
    "        image_info (dict): Dictionary containing Pillow image.\n",
    "        dataset_name (str): The name of the dataset.\n",
    "        image_index (int): The index of the image within the dataset.\n",
    "        resize (bool): A flag indicating whether the image should be resized.\n",
    "        resize_dim (int): The dimension to resize the image to if resizing is enabled.\n",
    "        use_prompts (bool): A flag indicating whether prompts should be used in annotation generation.\n",
    "        verbose (int): The verbosity level for logging (0 = no logs/messages, 3 = most messages)\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the generated annotation and the time elapsed during processing.\n",
    "            If the image data is missing or annotation generation fails, returns (None, time_elapsed).\n",
    "    \"\"\"\n",
    "    if image_info['image'] is None:\n",
    "        if verbose > 1:\n",
    "            logging.debug(f\"Skipping image {image_index} in dataset {dataset_name} due to missing image data.\")\n",
    "        return None, 0\n",
    "\n",
    "    original_dimensions = image_info['image'].size\n",
    "    start_time = time.time()\n",
    "    annotation = generate_annotation(image_index,\n",
    "                                     dataset_name,\n",
    "                                     image_info['image'],\n",
    "                                     original_dimensions,\n",
    "                                     resize,\n",
    "                                     resize_dim,\n",
    "                                     use_prompts,\n",
    "                                     verbose)\n",
    "    save_annotation(dataset_dir, image_index, annotation, verbose)\n",
    "    time_elapsed = time.time() - start_time\n",
    "\n",
    "    if annotation == -1:\n",
    "        if verbose > 1:\n",
    "            logging.debug(f\"Failed to generate annotation for image {image_index} in dataset {dataset_name}\")\n",
    "        return None, time_elapsed\n",
    "    \n",
    "    return annotation, time_elapsed\n",
    "\n",
    "def compute_annotation_latency(processed_images: int, dataset_time: float, dataset_name: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the average annotation latency for a dataset.\n",
    "\n",
    "    This function calculates the average time taken to annotate each image in a dataset\n",
    "    based on the total time spent and the number of processed images. If no images were\n",
    "    processed, it returns 0.0.\n",
    "\n",
    "    Parameters:\n",
    "        processed_images (int): The number of images that were successfully processed.\n",
    "        dataset_time (float): The total time taken to process the dataset, in seconds.\n",
    "        dataset_name (str): The name of the dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: The average annotation latency per image in seconds. If no images were processed, returns 0.0.\n",
    "    \"\"\"\n",
    "    if processed_images > 0:\n",
    "        average_latency = dataset_time / processed_images\n",
    "        logging.info(f'Average annotation latency for {dataset_name}: {average_latency:.4f} seconds')\n",
    "        return average_latency\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e10f6-4f4c-4750-88e5-aee0bc8c74f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_annotation_dataset(real_image_datasets: List[Any],\n",
    "                                save_path: str = 'annotations/',\n",
    "                                verbose: int = 0,\n",
    "                                max_images: int | None = None,\n",
    "                                resize_images = False,\n",
    "                                resize_dim = 512,\n",
    "                                use_prompts : bool = True) -> Tuple[Dict[str, Dict[str, Any]], float]:\n",
    "    \"\"\"\n",
    "    Generates text annotations for images in the given datasets, saves them in a specified directory, \n",
    "    and computes the average per image latency. Returns a dictionary of new annotations and the average latency.\n",
    "\n",
    "    Parameters:\n",
    "        real_image_datasets (List[Any]): Datasets containing images.\n",
    "        save_path (str): Directory path for saving annotation files.\n",
    "        verbose (int): Verbosity level between 0, 1, 2, 3 for process logs/messages (No messages = 0; Most messages = 3).\n",
    "        max_images (int): Maximum number of images to annotate.\n",
    "        resize_images (bool): Allow image downsizing before captioning.\n",
    "                            Sets max dimensions to (resize_dim, resize_dim), maintaining aspect ratio.\n",
    "        use_prompts (bool): Whether to enable prompt input for BLIP-2 for more descriptive annotations.\n",
    "3\n",
    "    Returns:\n",
    "        Tuple[Dict[str, Dict[str, Any]], float]: A tuple containing the annotations dictionary and average latency.\n",
    "    \"\"\"\n",
    "    set_logging_level(verbose)\n",
    "    annotations_dir = ensure_save_path(save_path)\n",
    "    annotations = {}\n",
    "    total_time = 0\n",
    "    total_processed_images = 0\n",
    "\n",
    "    for i, dataset in enumerate(real_image_datasets):\n",
    "        dataset_name = dataset.huggingface_dataset_path\n",
    "        dataset_dir = create_annotation_dataset_directory(annotations_dir, dataset_name)\n",
    "        processed_images = 0\n",
    "        dataset_time = 0\n",
    "\n",
    "        for j, image_info in enumerate(dataset):\n",
    "            annotation, time_elapsed = \\\n",
    "            process_image(dataset_dir, image_info, dataset_name, j, resize_images, resize_dim, use_prompts, verbose)\n",
    "            if annotation is not None:\n",
    "                annotations.setdefault(dataset_name, {})[image_info['id']] = annotation\n",
    "                total_time += time_elapsed\n",
    "                dataset_time += time_elapsed\n",
    "                processed_images += 1\n",
    "                if max_images is not None and processed_images >= max_images:\n",
    "                    break\n",
    "\n",
    "        average_latency = compute_annotation_latency(processed_images, dataset_time, dataset_name)\n",
    "        total_processed_images += processed_images\n",
    "\n",
    "    overall_average_latency = total_time / total_processed_images if total_processed_images else 0\n",
    "    return annotations, overall_average_latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a8902-9de3-43da-863d-06bba2c6dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_datasets(base_dir):\n",
    "    \"\"\"List all subdirectories in the base directory.\"\"\"\n",
    "    return [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "\n",
    "def load_annotations(base_dir, dataset):\n",
    "    \"\"\"Load annotations from JSON files within a specified directory.\"\"\"\n",
    "    annotations = []\n",
    "    path = os.path.join(base_dir, dataset)\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            with open(os.path.join(path, filename), 'r') as file:\n",
    "                data = json.load(file)\n",
    "                annotations.append(data)\n",
    "    return annotations\n",
    "\n",
    "def load_diffuser(model_name):\n",
    "    \"\"\"Load a diffusion model by name, configured to provided arguments.\"\"\"\n",
    "    bt.logging.info(f\"Loading image generation model ({model_name})...\")\n",
    "    model = DiffusionPipeline.from_pretrained(\n",
    "        model_name, torch_dtype=torch.float32 if device == \"cpu\" else torch.float16, **DIFFUSER_ARGS[model_name]\n",
    "    )\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5c897b-b0b6-4dae-9263-5177c875ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPU\n",
    "def generate_images(annotations, diffuser, save_dir, num_images, batch_size, diffuser_name):\n",
    "    \"\"\"Generate images from annotations using a diffuser and save to the specified directory.\"\"\"\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    generated_images = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(min(num_images, len(annotations))):\n",
    "            start_loop = time.time()\n",
    "            annotation = annotations[i]\n",
    "            prompt = annotation['description']\n",
    "            index = annotation.get('index', f\"missing_index\")\n",
    "\n",
    "            logging.info(f\"Annotation {i}: {json.dumps(annotation, indent=2)}\")\n",
    "\n",
    "            generated_image = diffuser(prompt=prompt).images[0]\n",
    "            logging.info(f\"Type of generated image: {type(generated_image)}\")\n",
    "\n",
    "            if isinstance(generated_image, torch.Tensor):\n",
    "                img = ToPILImage()(generated_image)\n",
    "            else:\n",
    "                img = generated_image\n",
    "\n",
    "            safe_prompt = prompt[:50].replace(' ', '_').replace('/', '_').replace('\\\\', '_')\n",
    "            img_filename = f\"{save_dir}/{safe_prompt}-{index}.png\"\n",
    "            img.save(img_filename)\n",
    "            generated_images.append(img_filename)\n",
    "            loop_time = time.time() - start_loop\n",
    "            logging.info(f\"Image saved to {img_filename}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    logging.info(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "    return generated_images\n",
    "\n",
    "\n",
    "def load_and_initialize_diffuser(diffuser_name, previous_diffuser=None):\n",
    "    \"\"\"Load and initialize the diffuser, handling previous diffuser cleanup if needed.\"\"\"\n",
    "    if previous_diffuser is not None:\n",
    "        logging.info(\"Deleting previous diffuser, freeing memory\")\n",
    "        # Move to float32 if it's float16, then move to CPU for deletion\n",
    "        if previous_diffuser.dtype == torch.float16:\n",
    "            previous_diffuser = previous_diffuser.to(dtype=torch.float32)\n",
    "        previous_diffuser.to('cpu')\n",
    "        del previous_diffuser\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return load_diffuser(diffuser_name)\n",
    "\n",
    "def run_diffuser_on_dataset(dataset, annotations, diffuser, output_dir, num_images, batch_size, diffuser_name):\n",
    "    \"\"\"Test a single diffuser on a given dataset.\"\"\"\n",
    "    dataset_name = dataset.rsplit('/', 1)[-1] if '/' in dataset else dataset\n",
    "    diffuser_name = diffuser_name.rsplit('/', 1)[-1] if '/' in diffuser_name else diffuser_name\n",
    "    save_dir = os.path.join(output_dir, dataset_name, diffuser_name)\n",
    "    logging.info(f\"Testing {diffuser_name} on annotation dataset {dataset} at {save_dir}...\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        generate_images(annotations, diffuser, save_dir, num_images, batch_size, diffuser_name)\n",
    "        logging.info(\"Images generated and saved successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to generate images with {diffuser_name}: {str(e)}\")\n",
    "\n",
    "def cleanup_diffuser(diffuser):\n",
    "    \"\"\"Clean up resources associated with a diffuser.\"\"\"\n",
    "    logging.info(\"Deleting diffuser, freeing memory\")\n",
    "    # Move to float32 if it's float16, then move to CPU for deletion\n",
    "    if diffuser.dtype == torch.float16:\n",
    "        diffuser = diffuser.to(dtype=torch.float32)\n",
    "    diffuser.to('cpu')\n",
    "    del diffuser\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def run_diffusers_on_datasets(annotations_dir, output_dir, num_images=sys.maxsize, batch_size=2):\n",
    "    \"\"\"Test all diffusers on datasets.\"\"\"\n",
    "    datasets = list_datasets(annotations_dir)\n",
    "    for diffuser_name in DIFFUSER_NAMES:\n",
    "        logging.info(f\"Loading and initializing diffuser: {diffuser_name}\")\n",
    "        diffuser = load_and_initialize_diffuser(diffuser_name)\n",
    "        for dataset in datasets:\n",
    "            annotations = load_annotations(annotations_dir, dataset)\n",
    "            run_diffuser_on_dataset(dataset, annotations, diffuser, output_dir, num_images, batch_size, diffuser_name)\n",
    "        cleanup_diffuser(diffuser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af075cf7-7287-46cd-89f1-c3c79df154cc",
   "metadata": {},
   "source": [
    "### Generate Annotations\n",
    "\n",
    "Change real_image_datasets to select which datasets to generate annotations and synthetics from.\n",
    "\n",
    "max_images=None for entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ade8f4-ecb6-407e-811d-3bf6acd1255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATIONS_DIR = \"test_data/dataset/annotations/\"\n",
    "OUTPUT_DIR = \"test_data/dataset/synthetics_from_annotations/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea35109c-9e92-41a7-8432-a42aa20116dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Loading real datasets\")\n",
    "real_image_datasets = [\n",
    "    ImageDataset(ds['path'], 'test', ds.get('name', None), ds['create_splits'])\n",
    "    for ds in DATASET_META['real']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab83535c-c67c-40ab-a0bc-e47d90c63031",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in real_image_datasets:\n",
    "    print(dataset.huggingface_dataset_path, dataset.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b671b-d2b2-423b-8305-74668fcd4130",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "annotations_dict, average_latency = generate_annotation_dataset(real_image_datasets,\n",
    "                                                                save_path=ANNOTATIONS_DIR,\n",
    "                                                                verbose=2,\n",
    "                                                                max_images=None,\n",
    "                                                               use_prompts=True)\n",
    "# move BLIP-2 to cpu and free up\n",
    "model.cpu()\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaac1ae-1bf1-41e8-b898-e27d7d62a7d2",
   "metadata": {},
   "source": [
    "### Generate Synthetic Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb165f29-0474-4df3-8e6d-dc25e5e8f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# Suppress FutureWarnings from diffusers module\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module='diffusers')\n",
    "# Set device for model operations\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "if device == \"cpu\":\n",
    "    raise RuntimeError(\"This script requires a GPU because it uses torch.float16.\")  # Added check for GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c801e9-6855-4aef-9282-a0c346962a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_diffusers_on_datasets(ANNOTATIONS_DIR, OUTPUT_DIR, num_images=sys.maxsize, batch_size=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
